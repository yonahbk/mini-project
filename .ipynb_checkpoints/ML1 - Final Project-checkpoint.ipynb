{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitted by:\n",
    " \n",
    "* ID 1: \n",
    "* ID 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm up - KNN\n",
    "\n",
    "Consider the [0,1] x [0,1] unit square. In this square we have points of several colors (classes). The goal is determine which color should be used to paint each of the non-painted points in the square using KNN.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make matplotlib figures appear inline in the notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Make the notebook automatically reload external python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color palette\n",
    "test_colors = ['dodgerblue', 'yellow', 'lightgreen']\n",
    "train_colors = ['blue', 'orange', 'green']\n",
    "\n",
    "def generate_data(N, cluster_type):\n",
    "    \"\"\"\n",
    "    Generate 3 clusters of points each of size N. These clusters can be\n",
    "    either separated, slightly mixed, or completely random.\n",
    "    \n",
    "    Args:\n",
    "        N              - number of points in a single cluster\n",
    "        cluster_type   - 'separated', 'mixed' or 'random'\n",
    "        \n",
    "    Return:\n",
    "        X_train - numpy array of dim (3N,2) that holds (x,y) point coordinates \n",
    "        y_train - numpy array of dim (3N,) that holds labels (0,1, or 2)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed=50)\n",
    "    \n",
    "    if cluster_type == 'separated':\n",
    "        x1 = 1 + 3*np.random.rand(N)\n",
    "        y1 = 1 + 3*np.random.rand(N)\n",
    "\n",
    "        x2 = 7 + 4*np.random.rand(N)\n",
    "        y2 = 2 + 4*np.random.rand(N)\n",
    "\n",
    "        x3 = 14 + 2*np.random.rand(N)\n",
    "        y3 = 6  + 5*np.random.rand(N)\n",
    "        \n",
    "    elif cluster_type == 'mixed':\n",
    "        x1 = 1 + 3*np.random.rand(N)\n",
    "        y1 = 1 + 3*np.random.rand(N)\n",
    "\n",
    "        x2 = 2 + 4*np.random.rand(N)\n",
    "        y2 = 2 + 4*np.random.rand(N)\n",
    "\n",
    "        x3 = 3 + 5*np.random.rand(N)\n",
    "        y3 = 3 + 5*np.random.rand(N)\n",
    "        \n",
    "    elif cluster_type == 'random':\n",
    "        x1 = np.random.rand(N)\n",
    "        y1 = np.random.rand(N)\n",
    "\n",
    "        x2 = np.random.rand(N)\n",
    "        y2 = np.random.rand(N)\n",
    "\n",
    "        x3 = np.random.rand(N)\n",
    "        y3 = np.random.rand(N)\n",
    "    else:\n",
    "        return (None, None)\n",
    "    \n",
    "    # normalizing so that points are in the [0,1]x[0,1] box\n",
    "    max_x = max(max(x1), max(x2), max(x3))\n",
    "    x1 /= max_x\n",
    "    x2 /= max_x\n",
    "    x3 /= max_x\n",
    "    max_y = max(max(y1), max(y2), max(y3))\n",
    "    y1 /= max_y\n",
    "    y2 /= max_y\n",
    "    y3 /= max_y\n",
    "        \n",
    "\n",
    "    # preparing X_train by concatenating the three sets of points\n",
    "    col1 = np.concatenate((x1, x2, x3))  # array of shape (3N,)\n",
    "    col2 = np.concatenate((y1, y2, y3))  # array of shape (3N,)\n",
    "\n",
    "    # reshaping into (N,2)\n",
    "    col1 = col1.reshape(-1,1)                   # -1 means whichever value that make the reshape work\n",
    "    col2 = col2.reshape(-1,1)\n",
    "\n",
    "    X_train = np.column_stack((col1, col2))\n",
    "    X_train = np.around(X_train,2)              # rounding to two decimals\n",
    "\n",
    "    # preparing y_train by setting the proper labels \n",
    "    y_train = np.ones (3*N)\n",
    "    y_train[0:N] -= 1 \n",
    "    y_train[2*N:3*N] += 1\n",
    "\n",
    "    return (X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N= 70\n",
    "#X_train, y_train = generate_data(N, 'separated')\n",
    "X_train, y_train = generate_data(N, 'random')\n",
    "#X_train, y_train = generate_data(N, 'mixed')\n",
    "\n",
    "print ('X_train shape: ', X_train.shape)\n",
    "print ('X_train first 10 points:')\n",
    "print (X_train[0:10,])\n",
    "print()\n",
    "\n",
    "print ('y_train shape: ', y_train.shape)\n",
    "#print ('y_train first 10 points:')\n",
    "#print (y_train[0:10,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting train points\n",
    "for i in range(3):\n",
    "    a_trn = X_train[np.where(y_train ==i)][:,0]  # grab X_train points for which label is i. grab x coordinate\n",
    "    b_trn = X_train[np.where(y_train ==i)][:,1]  # grab X_train points for which label is i. grab y coordinate \n",
    "    plt.scatter(a_trn, b_trn, color=train_colors[i], label = i,)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 100\n",
    "X_test = np.ones((resolution*resolution, 2))\n",
    "for i in np.arange(0,resolution,):\n",
    "    for j in np.arange(0,resolution):\n",
    "        X_test[i*resolution + j,0] = i/resolution   \n",
    "        X_test[i*resolution + j,1] = j/resolution\n",
    "        \n",
    "print ('Shape of X_test:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use KNN to paint the whole unit square\n",
    "\n",
    "1. Define a nearset neighbor classifier using KNeighborsClassifier and use it along with the training data to produce prediction for the given test points (X_test). \n",
    "2. Place your precdiction in the variable y_pred.\n",
    "3. Plot below both train and test. See if result makes sense.\n",
    "\n",
    "Repeat the above for the three modes of training data - 'separated', 'random', 'mixed'. \n",
    "1. For 'mixed' mode, try K=1 and K=9. Can you see a difference?\n",
    "2. Use K=3, 'mixed' data with both L1 and L2 distances (i.e., Manhattan and Euclidean distances, respectively). Can you see a difference in the boundary shape?\n",
    "\n",
    "For each combination of data and hyper-parameters plot a graph. In the graph title, include the hyper-parameters and the data type ('separated', 'random', 'mixed')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 8, 5  # width, height\n",
    "\n",
    "#plotting test and points:\n",
    "for i in range(3):\n",
    "    a_trn = X_train[np.where(y_train ==i)][:,0]  # grab X_train points for which label is i. grab the x coordinate\n",
    "    b_trn = X_train[np.where(y_train ==i)][:,1]  # grab X_train points for which label is i. grab the y coordinate\n",
    "    a_tst = X_test[np.where(y_pred ==i)][:,0]    # grab X_train points for which label is i. grab the x coordinate\n",
    "    b_tst = X_test[np.where(y_pred ==i)][:,1]    # grab X_train points for which label is i. grab the y coordinate\n",
    "    \n",
    "    plt.scatter(a_tst, b_tst, color=test_colors[i])\n",
    "    plt.scatter(a_trn, b_trn, color=train_colors[i], label = i,)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrospective\n",
    "\n",
    "Check the results. Do they make sense? Explain the visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm up - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for ploting the decision boundaries of a model\n",
    "# You will use it later\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.01):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('.', '.')\n",
    "    colors = ('blue', 'red')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.8, \n",
    "                    c=colors[idx],\n",
    "                    marker=markers[idx], \n",
    "                    label=cl, \n",
    "                    edgecolor='black')\n",
    "        \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(25)\n",
    "X, Y = make_classification(\n",
    "    n_samples=1000, n_features=2, n_redundant=0, n_informative=1, n_clusters_per_class=1, flip_y=0.05\n",
    ")\n",
    "\n",
    "colors = ['r' if x else 'g' for x in Y]\n",
    "plt.scatter(X[:, 0], X[:, 1], marker=\"o\", c=colors, s=25, edgecolor=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given an already implemented logistic regression class. Understand the code and use it in the next part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionGD(object):\n",
    "    \"\"\"\n",
    "    Logistic Regression Classifier using gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "      Passes over the training dataset.\n",
    "    eps : float\n",
    "      minimal change in the cost to declare convergence\n",
    "    random_state : int\n",
    "      Random number generator seed for random weight\n",
    "      initialization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eta=0.00005, n_iter=10000, eps=0.000001, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.eps = eps\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" \n",
    "        Fit training data (the learning phase).\n",
    "        Updating the theta vector in each iteration using gradient descent.\n",
    "        Store the theta vector in an attribute of the LogisticRegressionGD object.\n",
    "        Stop the function when the difference between the previous cost and the current is less than eps\n",
    "        or when you reach n_iter.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_examples, n_features]\n",
    "          Training vectors, where n_examples is the number of examples and\n",
    "          n_features is the number of features.\n",
    "        y : array-like, shape = [n_examples]\n",
    "          Target values.\n",
    "\n",
    "        \"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.theta = np.random.normal(loc=0.0, scale=1, size=1 + X.shape[1])\n",
    "        self.cost = []\n",
    "        old_cost = 0\n",
    "        output = []\n",
    "        for i in range(self.n_iter):\n",
    "            net_input = self.net_input(X)\n",
    "            output = self.activation(net_input)\n",
    "            errors = (output - y)\n",
    "            self.theta[1:] -= self.eta * X.T.dot(errors)\n",
    "            self.theta[0] -= self.eta * errors.sum()\n",
    "            # note that we compute the logistic `cost` now\n",
    "            # instead of the sum of squared errors cost\n",
    "            cost = (-y.dot(np.log(output)) - ((1 - y).dot(np.log(1 - output)))).sum() / X.shape[0]\n",
    "            if abs(cost - old_cost) < self.eps:\n",
    "                break\n",
    "            old_cost = cost\n",
    "            self.cost.append(cost)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return the predicted class label\"\"\"\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        \"\"\"Return the predicted class label\"\"\"\n",
    "        return self.activation(self.net_input(X))\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.theta[1:]) + self.theta[0]\n",
    "\n",
    "    def activation(self, z):\n",
    "        \"\"\"Compute logistic sigmoid activation\"\"\"\n",
    "        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "Use 5-fold cross validation in order to find the best eps and eta params from the given lists.\n",
    "\n",
    "For this part, you can use a straightforward approach and loop on all possible hyper-parameter combinations, or use dedicated functions from `sklearn`.\n",
    "\n",
    "Shuffle the training set before you split the data to the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "etas = [0.05, 0.005, 0.0005, 0.00005, 0.000005]\n",
    "epss = [0.01, 0.001, 0.0001, 0.00001, 0.000001]\n",
    "\n",
    "#### Your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "1. Build a model with the best params that you found in the previous section.\n",
    "1. Fit the model to the entire dataset.\n",
    "1. Print the cross validation score.\n",
    "1. Use the `plot_decision_regions` function to plot the decision boundaries.\n",
    "1. Plot the model cost as a function of the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 3-classes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(28)\n",
    "X, Y = make_classification(\n",
    "    n_samples=1000, n_features=2, n_redundant=0, n_informative=2, n_classes=3, n_clusters_per_class=1, flip_y=0.05\n",
    ")\n",
    "colors = ['r' if x==2 else 'y' if x==1 else 'b' for x in Y]\n",
    "plt.scatter(X[:, 0], X[:, 1], marker=\"o\", c=colors, s=25, edgecolor=\"k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 1-vs-all classifier\n",
    "\n",
    "1. Notice the `predict_prob` function in the logistic regression class. This functions returns the \"probability\" to be in the positive class.\n",
    "1. For each class, create 1-vs-all classifier (a single class would be positive, the rest would be negative) and use the 3 classifiers to predict the class of any instance.\n",
    "1. Recall that an instance $x$ will be predicted to class $i$ if $P(x$ belongs to class $i) > P(x$ belongs to class $j)\\ \\forall j \\neq i$\n",
    "1. Print the training accuracy.\n",
    "1. Store the models in the models variable - you will use it in the evaluation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.zeros((Y.shape[0], 3))\n",
    "models = []\n",
    "#### Your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the models variable and the `plot_decision_regions_3_classes` to plot the decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for ploting the decision boundaries of a model\n",
    "def plot_decision_regions_3_classes(X, y, models, resolution=0.01):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('.', '.', '.')\n",
    "    colors = ('b', 'y', 'r')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = np.zeros((np.array([xx1.ravel(), xx2.ravel()]).T.shape[0], 3))\n",
    "    Z[:, 0] = models[0].predict_prob(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z[:, 1] = models[1].predict_prob(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z[:, 2] = models[2].predict_prob(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.argmax(axis=1)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.8, \n",
    "                    c=colors[idx],\n",
    "                    marker=markers[idx], \n",
    "                    label=cl, \n",
    "                    edgecolor='black')\n",
    "        \n",
    "    return Z\n",
    "\n",
    "plot_decision_regions_3_classes(X, Y, models)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic survival prediction\n",
    "\n",
    "\n",
    "\n",
    "In this part we will attempt to predict who survived on the Titanic. The data set we use has the following vairbales (features/attributes):\n",
    "\n",
    "\n",
    "|  Variable   |          Definition          |              Key/Values                 |\n",
    "|:-----------:|:----------------------------:|:---------------------------------------:|\n",
    "| PassengerId | Index                        |integer                                  |\n",
    "| Pclass      | Ticket class                 |1=1st, 2=2nd, 3=3rd                      |\n",
    "| Name        | Name of passenger            |string                                   |\n",
    "| Sex         | Sex                          |male, female                             |\n",
    "| Age         | Age in years                 |integer                                  |\n",
    "| SibSp       | # of siblings/spouses aboard |integer                                  |\n",
    "| Parch       | # of parents/children aboard |integer                                  |\n",
    "| Ticket      | Ticket number                |string                                   |\n",
    "| Fare        | Ticket fare                  |float                                    |\n",
    "| Cabin       | Cabin number                 |a code                                   |\n",
    "| Embarked    | Port of Embarkation          |C=Cherbourg, Q=Queenstown, S=Southampton |\n",
    "| **Survived**| Predicted varibale           |0=No, 1=Yes                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:**\n",
    "\n",
    "The goal is to produce the best score possible.\n",
    "\n",
    "**Methodogology**\n",
    "\n",
    "So far you only two methods for classification: KNN and logistic regression. You can also use linear regression, but then you need to convert the output to 0s and 1s (this is not a straight forward use of linear regression but a possible one). You may want to choose which features to use (it could be that some features are not useful). Further, some features have missing values. You will need to decide how to handle this (for instance: drop rows with missing values, place an avergae value in those rows, or some other method of your choosing). Another matter to consider is handling non-numeric values. For example, sex is non-numeric. You may choose to drop non-numeric features, or you could convert them to numeric values (if such conversion makes sense).\n",
    "Also you will need to consider splitting the data into a training set and a test set so as to avoid tailoring the solution (overfitting) to the data you have.\n",
    "\n",
    "Bottom line: use everything we talked about in class in order to learn the best model.\n",
    "\n",
    "\n",
    "**Model Output**\n",
    "\n",
    "Your model needs to produce a prediciton for each data sample (row), which is 0 (did not survive) or 1 (survived). \n",
    "\n",
    "**Scoring your model**\n",
    "\n",
    "Your model performance will be assessed on test data that is not available to you (you will see the score next semester). Try to avoid overfitting.\n",
    "\n",
    "<br><br>\n",
    "**Final Note**\n",
    "\n",
    "This part of the project is open ended, in that you are not given small and specific tasks. However, you are already familiar with all the components needed to succeed. Specifically, reading data into pandas dataframe, dropping columns, dropping rows, changing value of features, splitting data into train and test subsets and performing model training and hyper parameter tuning via cross validation using sklearn library.\n",
    "\n",
    "GOOD LUCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
